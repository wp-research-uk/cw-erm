@inproceedings{codevilla2019exploring,
  title={Exploring the limitations of behavior cloning for autonomous driving},
  author={Codevilla, Felipe and Santana, Eder and L{\'o}pez, Antonio M and Gaidon, Adrien},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9329--9338},
  year={2019}
}

@inproceedings{causal-confusion,
 author = {de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Causal Confusion in Imitation Learning},
 volume = {32},
 year = {2019}
}

@article{three-regimes,
  author    = {Jonathan C. Spencer and
               Sanjiban Choudhury and
               Arun Venkatraman and
               Brian D. Ziebart and
               J. Andrew Bagnell},
  title     = {Feedback in Imitation Learning: The Three Regimes of Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/2102.02872},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2102.02872},
  timestamp = {Tue, 25 May 2021 15:56:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-02872.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vapnik1991,
 author = {Vapnik, V.},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Principles of Risk Minimization for Learning Theory},
 year = {1991}
}

@article{covshift,
title = {Improving predictive inference under covariate shift by weighting the log-likelihood function},
journal = {Journal of Statistical Planning and Inference},
volume = {90},
number = {2},
pages = {227-244},
year = {2000},
issn = {0378-3758},
doi = {https://doi.org/10.1016/S0378-3758(00)00115-4},
author = {Hidetoshi Shimodaira},
keywords = {Akaike information criterion, Design of experiments, Importance sampling, Kullback–Leibler divergence, Misspecification, Sample surveys, Weighted least squares},
abstract = {A class of predictive densities is derived by weighting the observed samples in maximizing the log-likelihood function. This approach is effective in cases such as sample surveys or design of experiments, where the observed covariate follows a different distribution than that in the whole population. Under misspecification of the parametric model, the optimal choice of the weight function is asymptotically shown to be the ratio of the density function of the covariate in the population to that in the observations. This is the pseudo-maximum likelihood estimation of sample surveys. The optimality is defined by the expected Kullback–Leibler loss, and the optimal weight is obtained by considering the importance sampling identity. Under correct specification of the model, however, the ordinary maximum likelihood estimate (i.e. the uniform weight) is shown to be optimal asymptotically. For moderate sample size, the situation is in between the two extreme cases, and the weight function is selected by minimizing a variant of the information criterion derived as an estimate of the expected loss. The method is also applied to a weighted version of the Bayesian predictive density. Numerical examples as well as Monte-Carlo simulations are shown for polynomial regression. A connection with the robust parametric estimation is discussed.}
}

@InProceedings{pmlr-v9-ross10a,
  title = 	 {Efficient Reductions for Imitation Learning},
  author = 	 {Ross, Stephane and Bagnell, Drew},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {661--668},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/ross10a/ross10a.pdf},
  abstract = 	 {Imitation Learning, while applied successfully on many large real-world problems, is typically addressed as a standard supervised learning problem, where it is assumed the training and testing data are i.i.d..  This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner’s policy is slowly modified from executing the expert’s policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively.}
}

@inproceedings{resampling-outperforms,
  author    = {Jing An and
               Lexing Ying and
               Yuhua Zhu},
  title     = {Why resampling outperforms reweighting for correcting sampling bias
               with stochastic gradients},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  biburl    = {https://dblp.org/rec/conf/iclr/AnYZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vitelli2022safetynet,
  title={Safetynet: Safe planning for real-world self-driving vehicles using machine-learned policies},
  author={Vitelli, Matt and Chang, Yan and Ye, Yawei and Ferreira, Ana and Wo{\l}czyk, Maciej and Osi{\'n}ski, B{\l}a{\.z}ej and Niendorf, Moritz and Grimmett, Hugo and Huang, Qiangui and Jain, Ashesh and others},
  booktitle={2022 International Conference on Robotics and Automation (ICRA)},
  pages={897--904},
  year={2022},
  organization={IEEE}
}

@inproceedings{transformers_all_you_need,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@inproceedings{gao2020vectornet,
  title={Vectornet: Encoding hd maps and agent dynamics from vectorized representation},
  author={Gao, Jiyang and Sun, Chen and Zhao, Hang and Shen, Yi and Anguelov, Dragomir and Li, Congcong and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11525--11533},
  year={2020}
}

@InProceedings{qi2016pointnet,
author = {Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
title = {Pointnet: Deep learning on point sets for 3d classification and segmentation},
booktitle = {CVPR},
year = {2017},
}
