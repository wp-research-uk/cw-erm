---
layout: distill
title: "CW-ERM: Improving Autonomous Driving Planning with Closed-loop Weighted Empirical Risk Minimization"
description: Improving Autonomous Driving Planning with Closed-loop Weighted Empirical Risk Minimization
date: 2022-10-25

authors:
  - name: Eesha Kumar *
    url:
    affiliations:
      name: Woven Planet United Kingdom Limited
  - name: Yiming Zhang
    url:
    affiliations:
      name: Woven Planet North America, Inc.
  - name: Stefano Pini
    url:
    affiliations:
      name: Woven Planet United Kingdom Limited
  - name: Simon Stent
    url:
    affiliations:
      name: Woven Planet United Kingdom Limited
  - name: Ana Ferreira
    url:
    affiliations:
      name: Woven Planet North America, Inc.
  - name: Sergey Zagoruyko
    url:
    affiliations:
      name: Woven Planet United Kingdom Limited
  - name: "Christian S. Perone * <br/><br/> * Equal contribution."
    url:
    affiliations:
      name: Woven Planet United Kingdom Limited

bibliography: robusttrain.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Closed-loop Weighted Empirical Risk Minimization (CW-ERM)
  - name: Interesting connection with covariate shift adaptation with density ratio estimation
  - name: Experimental evaluation
  - name: Discussion
  - name: Acknowledgement

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

  header {
    display: none;
  }

  d-byline .authors-affiliations {
    grid-column-end: span 3;
    grid-template-columns: 1fr 1fr;
    margin-bottom: 1em;
  }
---

<a href="https://arxiv.org/abs/2210.02174" >
      <button type="button" class="btn btn-primary btn-lg"><i class="fas fa-sticky-note"></i> ArXiv Paper</button>
</a> 
<!--button type="button" class="btn btn-secondary btn-lg">Large button</button-->

## Introduction
Imitation Learning (IL), and especially Behavioral Cloning (BC) are widely used today for many tasks. BC is of especial interest because it can take advantage of the supervised learning properties such as sample complexity and learning from historical data (demonstrations). Behavioral Cloning (BC), however, still face fundamental challenges <d-cite key="codevilla2019exploring"></d-cite>, including causal confusion <d-cite key="causal-confusion"></d-cite> (later identified as a feedback-driven covariate shift <d-cite key="three-regimes"></d-cite>) and dataset biases <d-cite key="codevilla2019exploring"></d-cite>, to name a few.

One of the main issues of BC that is often overlooked, especially in policies trained for autonomous vehicles (AVs) is the mismatch between the training and inference-time distributions. Usually, BC policies are trained in an open-loop fashion, predicting the next action given the immediate previous action and optionally conditioned on recent past actions. This is quite different than the way that these policies are evaluated and deployed. During test time, the policy is used not only to predict an action, but this action is also execute in the simulated or real-world environment and impacts the next state. This difference is explained in the image below:

<div class="row mt-3 l-page">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/ol_vs_cl.png" class="img-fluid rounded" %}
    </div>
</div>
<div class="caption l-page">
    Figure 1: comparison between Open-loop and Closed-loop training/evaluation of driving policies.
</div>

When executed in real-world (or even in a simulated environment), small predictions errors can drive covariate shift and make the network predict in an out-of-distribution regime. In the animation below we can see an example of a policy that starts to diverge and is unable to recover:

<div class="row mt-3 l-body">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/diverge_loop.gif" class="img-fluid rounded" %}
    </div>
</div>
<div class="caption l-body">
    Figure 2: animation showing a policy that starts to diverge and is unable to recover.
</div>

If we imagine a State-Action manifold (SxA) like in the figure below, we can see how the state-action datapoints start to diverge from the data that the model was trained on: 

<div class="row mt-3 l-body">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/state-action-manifold.gif" class="img-fluid rounded" %}
    </div>
</div>
<div class="caption l-body">
    Figure 3: animation showing how small errors can make the policy diverge outside of the training
    state-action manifold, this making the model predict in out-of-distribution regime.
</div>

In this work, we address the mismatch between training and inference mentioned above through the development of a simple training principle. Using a closed-loop simulator, we first identify and then reweight samples that are important for the closed-loop performance of the policy. We call this approach <strong>CW-ERM</strong> (Closed-loop Weighted Empirical Risk Minimization), since we use Weighted ERM <d-cite key="covshift"></d-cite> to correct the training distribution in favour of closed-loop performance. We extensively evaluate this principle on real-world urban driving data and show that it can achieve significant improvements on planner metrics that matter for real-world performance (e.g. collisions).

<div class="card border shadow-0 mb-3 bg-white" style="font-weight: 1;">
  <div class="card-header">Contributions</div>
  <div class="card-body">
    <p class="card-text">
    <ul>
      <li>We motivate and propose Closed-loop Weighted Empirical Risk Minimization (CW-ERM), a technique that leverages closed-loop evaluation metrics acquired from policy rollouts in a simulator to debias the policy network and reduce the distributional differences between training (open-loop) and inference time (closed-loop);</li>
      <li>We evaluate CW-ERM experimentally on a challenging urban driving dataset in a closed-loop fashion to show that our method, although simple to implement, yields significant improvements in closed-loop performance without requiring complex and computationally expensive closed-loop training methods;</li>
      <li>We also show an important connection of our method to a family of methods that addresses covariate shift through density ratio estimation.</li>
      </ul>
    </p>
  </div>
</div>

## Closed-loop Weighted Empirical Risk Minimization (CW-ERM)

### Problem setup
The traditional formulation of supervised learning for imitation learning, also called behavioral cloning (BC), can be formulated as finding the policy $$\hat{\pi}_{BC}$$:

$$
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\begin{align}
\label{eqn:bc-erm}
\hat{\pi}_{BC} = \argmin_{\pi \in \Pi} \mathbb{E}_{s \sim d_{\pi^*}, a \sim \pi^*(s)}[\ell(s,a,\pi)]
\end{align}
$$

where the state $s$ is sampled from the expert state distribution $$d_{\pi^*}$$ induced when following the expert policy $$\pi^*$$. Actions $a$ are sampled from the expert policy $$\pi^*(s)$$. The loss $$\ell$$ is also known as the surrogate loss that will find the policy $$\hat{\pi}_{BC}$$ that best mimics the unknown expert policy $$\pi^*(s)$$. In practice, we only observe a finite set of state-action pairs $${(s_i, a^*_i)}_{i=1}^m$$, so the optimization is only approximate and we then follow the Empirical Risk Minimization (ERM) principle to find the policy $$\pi$$ from the policy class $$\Pi$$.

If we let $$\mathbb{E}_{s \sim d_{\pi^*}, a \sim \pi^*(s)}[\ell(s,a,\pi)] = \epsilon$$, then it follows that $$J(\pi) \leq J(\pi^*) + T^2 \epsilon$$ as shown by the proof in <d-cite key="pmlr-v9-ross10a"></d-cite>, where $$J$$ is the total cost and $$T$$ is the task horizon. As we can see, the total cost can grow quadratically in $$T$$.

When the policy $$\hat{\pi}_{BC}$$ is deployed in the real-world, it will eventually make mistakes and then induce a state distribution $$d_{\hat{\pi}_{BC}}$$ different than the one it was trained on ( $$d_{\pi^*}$$). During closed-loop evaluation of driving policies, non-imitative metrics such as collisions and comfort are also evaluated. However, they are often ignored in the surrogate loss or only implicitly learned by imitating the expert due to the difficulty of overcoming differentiability requirements, as smooth approximations of these metrics are still different than the non-differentiable counterparts often used. These policies can often show good results in open-loop training, but perform poorly in closed-loop evaluation or when deployed in a real SDV due to the differences between $$d_{\hat{\pi}_{BC}}$$ and $$d_{\pi^*}$$, where the estimator is no longer consistent.

### CW-ERM
In our method, called <strong>Closed-loop Weighted Empirical Risk Minimization (CW-ERM)</strong>, we seek to debias a policy network from the open-loop performance towards closed-loop performance, making the model rely on features that are robust to closed-loop evaluation. Our method consists of three stages: the training of an identification policy, the use of that policy in closed-loop simulation to identify samples, and the training of a final policy network on a reweighted data distribution. More explicitly:

#### Stage 1 (identification policy)
Train a traditional BC policy network in open-loop using ERM, to yield $$\hat{\pi}_{\text{ERM}}$$.

#### Stage 2 (closed-loop simulation)
Perform rollouts of the $$\hat{\pi}_{\text{ERM}}$$ policy in a closed-loop simulator, collect closed-loop metrics and then identify the error set below:

$$
\begin{align}
\label{eqn:error-set}
     E_{\hat{\pi}_{\text{ERM}}} = \{(s_i, a_i)~\text{s.t.}~ {C(s_i, a_i)} > 0 \},
\end{align}
$$

where $$\text{s}_i$$ is a training data sample, or scene with a fixed number of timesteps from the training set, $$\text{a}_i$$ is the action performed during the roll-out and $$C(\cdot)$$ is a cost such as the number of collisions found during closed-loop rollouts.

#### Stage 3 (final policy)
Train a new policy using weighted ERM where
the scenes belonging to the error set $$E_{\hat{\pi}_{\text{ERM}}}$$ are upweighted by a factor $$w(\cdot)$$, yielding the policy $$\hat{\pi}_{\text{CW-ERM}}$$:

$$
\begin{equation}
\label{eqn:bc-cw-erm}
\argmin_{\pi \in \Pi} \mathbb{E}_{s \sim d_{\pi^*}, a \sim \pi^*(s)}[w(E_{\hat{\pi}_{\text{ERM}}}, s) \ell(s,a,\pi)]
\end{equation}
$$

As we can see, the CW-ERM policy in Equation $$\ref{eqn:bc-cw-erm}$$ is very similar to the original BC policy trained with ERM in Equation $$\ref{eqn:bc-erm}$$, with the key difference of a weighting term based on the error set from closed-loop simulation in Stage 2. In practice, although statistically
equivalent, we upsample scenes by a fixed factor rather than reweighting, as it is known to be more stable and robust <d-cite key="resampling-outperforms"></d-cite>.

By training a policy using CW-ERM, we expect it to upsample scenes that perform poorly in closed-loop evaluation, making the policy network robust to the covariate shift seen during inference time while unrolling the policy. In the figure below you can see a visual overview of the steps involved into CW-ERM:

<div class="row mt-3 l-page">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/cwerm-diagram.png" class="img-fluid rounded" %}
    </div>
</div>
<div class="caption l-page">
    Figure 4: High-level overview of our proposed Closed-loop Weighted Empirical Risk Minimization (CW-ERM) method. In steps <strong>(1-2)</strong> we train an identification policy $\hat{\pi}_{\text{ERM}}$ using traditional ERM <d-cite key="vapnik1991"></d-cite> on a set of training data samples or driving "scenes". In step <strong>(3)</strong>, we perform closed-loop simulation of the policy $\hat{\pi}_{\text{ERM}}$ and collect metrics to construct the error set in step <strong>(4)</strong>. With the error set in hand, we upsample scenes in the training set as shown in step <strong>(5)</strong>. We train the final policy $\hat{\pi}_{\text{CW-ERM}}$ using CW-ERM as shown in step <strong>(6)</strong> with the upsampled $\mathcal{D}_{\text{up}}$ set.
</div>


### Interesting connection with covariate shift adaptation with density ratio estimation
One important connection of our method is with covariate shift correction using density ratio estimation <d-cite key="covshift"></d-cite>. To correct for the covariate shift, the negative log-likelihood is often weighted by the density ratio $$r(s)$$:

$$
\begin{equation}
\label{eqn:bc-density-ratio}
\argmin_{\pi \in \Pi} \mathbb{E}_{s \sim d_{\pi^*}, a \sim \pi^*(s)}[r(s) \ell(s,a,\pi)]
\end{equation}
$$

where $$r(s)$$ is defined as the density ratio between test and training distributions:

$$
\begin{equation}
\label{eqn:density-ratio}
r(s) = \frac{p_{\text{test}}(s)}{p_{\text{train}}(s)}
\end{equation}
$$

In practice, $$r(s)$$ is difficult to compute and is thus estimated. The density ratio will be higher when the sample is more important for the test distribution. In our method (CW-ERM), instead of using the density ratio to weight training samples, we resample the training set based on an estimate of each data point's importance towards good closed-loop behaviours. Like the density ratio, the weighting in our case will also be higher for when the sample is important for the test distribution.

One key characteristic of the importance weighted estimator is that it can be consistent even under covariate shift. We leave, however, the analysis of theoretical properties of our approximation for future work.


## Experimental evaluation

### Network architecture
Our method is agnostic to model architecture choices. To evaluate our CW-ERM approach, we adopt the recent network architecture of <d-cite key="vitelli2022safetynet"></d-cite> to represent a strong baseline performance for SDV planning, as we already tested and deployed in public roads. This model uses a transformer-based <d-cite key="transformers_all_you_need"></d-cite> architecture with a vectorial input representation <d-cite key="gao2020vectornet"></d-cite> to create features for each element into vector sets. It consists of a PointNet-based <d-cite key="qi2016pointnet"></d-cite> module for local processing of vectorized inputs and a global graph using a Transformer encoder for reasoning about interactions with agents and map features. Differently from <d-cite key="vitelli2022safetynet"></d-cite>, we don't use a safety layer, as we want to evaluate the planner performance without external trajectory fallbacks. For further details, please refer to <d-cite key="vitelli2022safetynet"></d-cite>.

<div class="row mt-3 l-page">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/arch.png" class="img-fluid rounded" %}
    </div>
</div>
<div class="caption l-page">
    Figure 5: overview of the network architecture employed to evaluate CW-ERM. Image from <d-cite key="vitelli2022safetynet"></d-cite>.
</div>


### Results

<div class="row mt-3 l-page">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/results.png" class="img-fluid rounded" %}
    </div>
</div>
<div class="caption l-page">
    Figure 6: Visual representation of the experimental results from closed-loop evaluation in simulation. Confidence intervals (CIs) were calculated using .95 interval from an exact Binomial posterior with a flat prior. In this plot we only compare against the best baseline (ERM with perturbation).
</div>

### Simulation samples

TODO

## Discussion
Most recent improvements in imitation learning are based on improving the asymptotic performance of algorithms. In this work we showed a different direction that tackles the problem by directly addressing the mismatch between training and inference without requiring an extra human oracle or adding extra complexity during training. Our method is as simple as upsampling scenes by leveraging any existing simulator and training two models, yet it showed that there is still room for significant improvements without having to deal with human-in-the-loop, training rollouts or impacting the policy inference latency. We also described an important potential connection of our method with density ratio estimation for covariate shift correction <d-cite key="covshift"></d-cite>, which we believe is an exciting future research direction that could provide better theoretical understanding of the improvements seen in our experiments.

## Acknowledgement
We would like to thank Kenta Miyahara, Nobuhiro Ogawa and Ezequiel Castellano for the review of this work and everyone from the UK Research Team and the ML Planning team who supported this work through the ecosystem needed for all experiments, and for the fruitful discussions.

***

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally youâ€™ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>